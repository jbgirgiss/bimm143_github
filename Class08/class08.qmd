---
title: "Class 8: Mini Project"
author: "Joseph Girgiss (PID: A17388247)"
format: pdf
toc: true
---

## Background

The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in class. You’ll extend what you’ve learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses. This expands on our RNA-Seq analysis from last day.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass.


## Data input

Data was downloaded from the class website as a CSV file. 

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)

# First 6 rows of data.frame
head(wisc.df)
```

```{r}
# We can use -1 here to remove the first column (remove patient ID)
wisc.data <- wisc.df[,-1]
```

We will create a diagnosis vector for use later when we compare information. 
```{r}
diagnosis <- factor(wisc.df$diagnosis)
```

## Exploratory data analysis

> Q1. How many observations are in this dataset?

There are `r nrow(wisc.data)` observations/patients in data set.  

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(wisc.df$diagnosis)
```
212 malignant diagnoses, and 357 benign diagnoses. 

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
length(grep("_mean$", colnames(wisc.data)))
```
## Principal Component Analysis (PCA)

The `prcomp()` function to do PCA has a `scale=FALSE` default. We want scale=TRUE so our analysis is not dominated by columns/variables in our dataset that have high standard deviation and mean when compared to others just because the units of measurement are different. 

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```


```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale = T)
```

```{r}
# Look at summary of results
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

Proportion of Variance for PC1 is 0.4427.

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs 

```{r}
which(summary(wisc.pr)$importance["Cumulative Proportion", ] >= 0.70)[1]
```

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs.

```{r}
which(summary(wisc.pr)$importance["Cumulative Proportion", ] >= 0.90)[1]
```

## Interpreting PCA results

The main PC result figure is called a "score plot" or "PC plot" or "ordination plot"

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

`biplot(wisc.pr)` is very difficult to read/understand. Too many points. 

```{r}
library(ggplot2)

#Scatter plot observations by components 1 and 2
ggplot(wisc.pr$x) +
  aes(PC1, PC2, col = diagnosis) +
  geom_point() +
  theme_bw()
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
ggplot(wisc.pr$x) +
  aes(PC1, PC3, col = diagnosis) +
  geom_point() +
  theme_bw()
```

The values in the PC1 and PC3 plots tend to be more negative than the PC1 and PC2 plot (therefore, PC1 and PC2 plot have more positive values than the PC1 and PC3 plot). There is more overlap between PC1 and PC3 than PC1 and PC2.

## Variance explained

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

## Communicating PCA results


> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
# Loading of 'concave.points_mean' on PC1
wisc.pr$rotation["concave.points_mean", "PC1"]
```

The feature concave.points_mean has a loading of −0.2608538 on the first principal component (PC1), indicating it has low influence on PC1. The negative sign means that higher values of concave.points_mean tend to decrease the PC1 score for an observation. Since the loading of concave.points_mean on PC1 is −0.2608538, higher values of this feature push PC1 toward the negative direction, which corresponds to malignant cases. 

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
# Cumulative proportion
cum.pve <- cumsum(pve)

# Find minimum number of PCs to reach 80%
which(cum.pve >= 0.80)[1]
```
5 PCs are needed to explain 80% of the variance of the data. 

## Hierarchical clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)

# Hierarchical clustering using complete linkage
wisc.hclust <- hclust(data.dist, method = "complete")
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

Height = 19 for when the clustering model has 4 clusters. 

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

```{r}
best_k <- 0
best_match <- 0

for(k in 2:10){
  clusters <- cutree(wisc.hclust, k = k)
  tbl <- table(clusters, diagnosis)
  match <- sum(apply(tbl, 1, max)) / length(diagnosis)
  
  if(match > best_match){
    best_match <- match
    best_k <- k
  }
}

cat("Best number of clusters:", best_k, "\n")
cat("Matching proportion:", best_match, "\n")
```
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 9)
table(wisc.hclust.clusters, diagnosis)
```

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

```{r}
# Hierarchical clustering using ward.D2
wisc.hclust.ward <- hclust(data.dist, method = "ward.D2")

# Cut tree into 2 clusters (since we have benign vs malignant)
ward.clusters <- cutree(wisc.hclust.ward, k = 2)

# Compare to actual diagnosis
table(ward.clusters, diagnosis)

# Optionally, compute proportion of correct matches
match_proportion <- sum(apply(table(ward.clusters, diagnosis), 1, max)) / length(diagnosis)
match_proportion
```

- The ward.D2 method minimizes the variance within clusters at each step, which tends to produce tight, spherical clusters that are more balanced in size. Complete linkage produces compact clusters but can overemphasize outliers, so ward.D2 is preferred.

## K-means clustering

```{r}
wisc.km <- kmeans(data.dist, centers = 2, nstart = 20)
table(wisc.km$cluster, diagnosis)
```

## Combinding Methods (PCA and Clustering)

Clustering the original data was not very productive. The PCA results looked promising. Here we combine these methods by clustering from our PCA results. In other words, "clustering in PC space"... not using the original data but our PCA data (PC1, PC2, PCn...). 

```{r}
# Take the first 3 PCs
dist.pc <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(dist.pc, method="ward.D2")
```

View the tree...
```{r}
plot(wisc.pr.hclust)
abline(h=70, col="red")
```

To get our clustering membership vector (i.e. our main clustering result) we "cut" the tree at a desired height or to yield a desired number of "k" groups. 

> Q15. How well does the newly created model with four clusters separate out the two diagnoses? 
- Note for 15/16: we did not perform k-means clustering in class and this section was optional. 

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps, diagnosis)
```

Separates well. Separates into true positives = 179, false negatives = 33, false positives = 24, true negatives = 333. 

> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses. How does this clustering grps compare to the expert diagnosis?


```{r}
table(wisc.km$cluster, diagnosis)
```

We did not perform k-means cluster in class. However, on my own, k-means clustering separated into true positives = 134, false negatives = 78, false positives = 20, true negatives = 337. Separate similarly, although h-clustering + PCA does a better job with sensitivity, specificity is slightly larger for k-means clustering.  

> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

We can calculate the sensitivity (how well h-clustering + PCA can detect positives/malignant). 

Sensitivity (h-clustering + PCA): TP/(TP+FN)
```{r}
179/(179+33)
```
We can calculate the specificity (how well h-clustering + PCA can detect negatives/benign). 

Specificity (h-clustering + PCA): TN/(TN+FP)
```{r}
333/(333 + 24)
```

Sensitivity (k-means clustering): TP/(TP+FN)
```{r}
134/(134+78)
```

Specificity (k-means clustering): TN/(TN+FP)
```{r}
337/(337 + 20)
```
H-clustering + PCA has a higher sensitivity (0.8443396 > 0.6320755), while specificity is slightly larger for k-means clustering (0.9439776 > 0.9327731).  

## Prediction

We can use our PCA model for prediction with new input patient samples. 

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
g <- factor(diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

- Should follow up with patient 2. 
















